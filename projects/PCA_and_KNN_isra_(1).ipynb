{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Utilities\n",
        "\n",
        "import numpy as np\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import scipy\n",
        "import scipy.ndimage\n",
        "import dlib\n",
        "\n",
        "\n",
        "def get_landmark(filepath, predictor):\n",
        "    \"\"\"get landmark with dlib\n",
        "    :return: np.array shape=(68, 2)\n",
        "    \"\"\"\n",
        "    detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "    img = dlib.load_rgb_image(filepath)\n",
        "    dets = detector(img, 1)\n",
        "\n",
        "    for k, d in enumerate(dets):\n",
        "        shape = predictor(img, d)\n",
        "\n",
        "    t = list(shape.parts())\n",
        "    a = []\n",
        "    for tt in t:\n",
        "        a.append([tt.x, tt.y])\n",
        "    lm = np.array(a)\n",
        "    return lm\n",
        "\n",
        "\n",
        "def align_face(filepath, predictor):\n",
        "    \"\"\"\n",
        "    :param filepath: str\n",
        "    :return: PIL Image\n",
        "    \"\"\"\n",
        "\n",
        "    lm = get_landmark(filepath, predictor)\n",
        "\n",
        "    lm_chin = lm[0: 17]  # left-right\n",
        "    lm_eyebrow_left = lm[17: 22]  # left-right\n",
        "    lm_eyebrow_right = lm[22: 27]  # left-right\n",
        "    lm_nose = lm[27: 31]  # top-down\n",
        "    lm_nostrils = lm[31: 36]  # top-down\n",
        "    lm_eye_left = lm[36: 42]  # left-clockwise\n",
        "    lm_eye_right = lm[42: 48]  # left-clockwise\n",
        "    lm_mouth_outer = lm[48: 60]  # left-clockwise\n",
        "    lm_mouth_inner = lm[60: 68]  # left-clockwise\n",
        "\n",
        "    # Calculate auxiliary vectors.\n",
        "    eye_left = np.mean(lm_eye_left, axis=0)\n",
        "    eye_right = np.mean(lm_eye_right, axis=0)\n",
        "    eye_avg = (eye_left + eye_right) * 0.5\n",
        "    eye_to_eye = eye_right - eye_left\n",
        "    mouth_left = lm_mouth_outer[0]\n",
        "    mouth_right = lm_mouth_outer[6]\n",
        "    mouth_avg = (mouth_left + mouth_right) * 0.5\n",
        "    eye_to_mouth = mouth_avg - eye_avg\n",
        "\n",
        "    # Choose oriented crop rectangle.\n",
        "    x = eye_to_eye - np.flipud(eye_to_mouth) * [-1, 1]\n",
        "    x /= np.hypot(*x)\n",
        "    x *= max(np.hypot(*eye_to_eye) * 2.0, np.hypot(*eye_to_mouth) * 1.8)\n",
        "    y = np.flipud(x) * [-1, 1]\n",
        "    c = eye_avg + eye_to_mouth * 0.1\n",
        "    quad = np.stack([c - x - y, c - x + y, c + x + y, c + x - y])\n",
        "    qsize = np.hypot(*x) * 2\n",
        "\n",
        "    # read image\n",
        "    img = PIL.Image.open(filepath)\n",
        "\n",
        "    output_size = 256\n",
        "    transform_size = 256\n",
        "    enable_padding = True\n",
        "\n",
        "    # Shrink.\n",
        "    shrink = int(np.floor(qsize / output_size * 0.5))\n",
        "    if shrink > 1:\n",
        "        rsize = (int(np.rint(float(img.size[0]) / shrink)), int(np.rint(float(img.size[1]) / shrink)))\n",
        "        try:\n",
        "            resample = Image.Resampling.LANCZOS\n",
        "        except AttributeError:\n",
        "            resample = Image.LANCZOS  # older versions\n",
        "        img = img.resize(rsize, resample)\n",
        "        quad /= shrink\n",
        "        qsize /= shrink\n",
        "\n",
        "    # Crop.\n",
        "    border = max(int(np.rint(qsize * 0.1)), 3)\n",
        "    crop = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n",
        "            int(np.ceil(max(quad[:, 1]))))\n",
        "    crop = (max(crop[0] - border, 0), max(crop[1] - border, 0), min(crop[2] + border, img.size[0]),\n",
        "            min(crop[3] + border, img.size[1]))\n",
        "    if crop[2] - crop[0] < img.size[0] or crop[3] - crop[1] < img.size[1]:\n",
        "        img = img.crop(crop)\n",
        "        quad -= crop[0:2]\n",
        "\n",
        "    # Pad.\n",
        "    pad = (int(np.floor(min(quad[:, 0]))), int(np.floor(min(quad[:, 1]))), int(np.ceil(max(quad[:, 0]))),\n",
        "           int(np.ceil(max(quad[:, 1]))))\n",
        "    pad = (max(-pad[0] + border, 0), max(-pad[1] + border, 0), max(pad[2] - img.size[0] + border, 0),\n",
        "           max(pad[3] - img.size[1] + border, 0))\n",
        "    if enable_padding and max(pad) > border - 4:\n",
        "        pad = np.maximum(pad, int(np.rint(qsize * 0.3)))\n",
        "        img = np.pad(np.float32(img), ((pad[1], pad[3]), (pad[0], pad[2]), (0, 0)), 'reflect')\n",
        "        h, w, _ = img.shape\n",
        "        y, x, _ = np.ogrid[:h, :w, :1]\n",
        "        mask = np.maximum(1.0 - np.minimum(np.float32(x) / pad[0], np.float32(w - 1 - x) / pad[2]),\n",
        "                          1.0 - np.minimum(np.float32(y) / pad[1], np.float32(h - 1 - y) / pad[3]))\n",
        "        blur = qsize * 0.02\n",
        "        img += (scipy.ndimage.gaussian_filter(img, [blur, blur, 0]) - img) * np.clip(mask * 3.0 + 1.0, 0.0, 1.0)\n",
        "        img += (np.median(img, axis=(0, 1)) - img) * np.clip(mask, 0.0, 1.0)\n",
        "        img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n",
        "        quad += pad[:2]\n",
        "\n",
        "    # Transform.\n",
        "    img = img.transform((transform_size, transform_size), PIL.Image.QUAD, (quad + 0.5).flatten(), PIL.Image.BILINEAR)\n",
        "    if output_size < transform_size:\n",
        "        img = img.resize((output_size, output_size), PIL.Image.ANTIALIAS)\n",
        "\n",
        "    # Return aligned image.\n",
        "    return img"
      ],
      "metadata": {
        "cellView": "form",
        "id": "YXvR_nYDwMa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7ygz-9WY-Wl",
        "outputId": "13383a9a-85d5-474e-a0e6-aca755cf7539"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyDrive\n",
            "  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/987.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.5/987.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.11/dist-packages (from PyDrive) (2.177.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.11/dist-packages (from PyDrive) (6.0.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.2->PyDrive) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.2->PyDrive) (2.25.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.2->PyDrive) (4.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.9.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.2->PyDrive) (5.5.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client>=1.2->PyDrive) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2025.8.3)\n",
            "Building wheels for collected packages: PyDrive\n",
            "  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyDrive: filename=PyDrive-1.3.1-py3-none-any.whl size=27433 sha256=93f3ef0e4faebfaefaae9c412b0e459ec6499767bbf8d16c09f7b842feb00520\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/d5/09/88865e0059104686eb8365ca1d36a8b27deef34232c3b62c90\n",
            "Successfully built PyDrive\n",
            "Installing collected packages: PyDrive\n",
            "Successfully installed PyDrive-1.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYcPSBrtjGxb",
        "outputId": "e4a70943-c9c7-42ab-854b-7ecb5be6c73d",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'encoder4editing'...\n",
            "remote: Enumerating objects: 172, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 172 (delta 49), reused 42 (delta 42), pack-reused 94 (from 1)\u001b[K\n",
            "Receiving objects: 100% (172/172), 33.43 MiB | 33.93 MiB/s, done.\n",
            "Resolving deltas: 100% (59/59), done.\n",
            "--2025-08-09 09:57:00--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-09T10%3A50%3A36Z&rscd=attachment%3B+filename%3Dninja-linux.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-09T09%3A49%3A45Z&ske=2025-08-09T10%3A50%3A36Z&sks=b&skv=2018-11-09&sig=gJ79fyLToQzBIadQYjecygRM2AEr0Vkac58j015EmXU%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NDczMzcyMCwibmJmIjoxNzU0NzMzNDIwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.wxVjciznpCTFTWxRKNbu1Jht2puc6fTieRQeH9LZreU&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-08-09 09:57:00--  https://release-assets.githubusercontent.com/github-production-release-asset/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-08-09T10%3A50%3A36Z&rscd=attachment%3B+filename%3Dninja-linux.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-08-09T09%3A49%3A45Z&ske=2025-08-09T10%3A50%3A36Z&sks=b&skv=2018-11-09&sig=gJ79fyLToQzBIadQYjecygRM2AEr0Vkac58j015EmXU%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1NDczMzcyMCwibmJmIjoxNzU0NzMzNDIwLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.wxVjciznpCTFTWxRKNbu1Jht2puc6fTieRQeH9LZreU&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ‘ninja-linux.zip’\n",
            "\n",
            "ninja-linux.zip     100%[===================>]  76.03K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-08-09 09:57:00 (4.16 MB/s) - ‘ninja-linux.zip’ saved [77854/77854]\n",
            "\n",
            "Archive:  ninja-linux.zip\n",
            "  inflating: /usr/local/bin/ninja    \n",
            "update-alternatives: using /usr/local/bin/ninja to provide /usr/bin/ninja (ninja) in auto mode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1cUv_reLE6k3604or78EranS7XzuVMWeO\n",
            "From (redirected): https://drive.google.com/uc?id=1cUv_reLE6k3604or78EranS7XzuVMWeO&confirm=t&uuid=868ffd93-4f91-46ef-9785-339691141e5f\n",
            "To: /content/encoder4editing/pretrained_models/e4e_ffhq_encode.pt\n",
            "100% 1.20G/1.20G [00:13<00:00, 88.3MB/s]\n",
            "--2025-08-09 09:59:05--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 [following]\n",
            "--2025-08-09 09:59:05--  https://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64040097 (61M)\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat.bz2’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  61.07M  32.5MB/s    in 1.9s    \n",
            "\n",
            "2025-08-09 09:59:08 (32.5 MB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Setup Repository\n",
        "import os\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'encoder4editing'\n",
        "\n",
        "!git clone https://github.com/omertov/encoder4editing.git $CODE_DIR\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "\n",
        "from argparse import Namespace\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from utils.common import tensor2im\n",
        "from models.psp import pSp  # we use the pSp framework to load the e4e encoder.\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "#@title Setup files downloader\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "download_with_pydrive = False #@param {type:\"boolean\"}\n",
        "\n",
        "class Downloader(object):\n",
        "    def __init__(self, use_pydrive):\n",
        "        self.use_pydrive = use_pydrive\n",
        "        current_directory = os.getcwd()\n",
        "        self.save_dir = os.path.join(os.path.dirname(current_directory), CODE_DIR, \"pretrained_models\")\n",
        "        os.makedirs(self.save_dir, exist_ok=True)\n",
        "        if self.use_pydrive:\n",
        "            self.authenticate()\n",
        "\n",
        "    def authenticate(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        self.drive = GoogleDrive(gauth)\n",
        "\n",
        "    def download_file(self, file_id, file_name):\n",
        "        file_dst = f'{self.save_dir}/{file_name}'\n",
        "        if os.path.exists(file_dst):\n",
        "            print(f'{file_name} already exists!')\n",
        "            return\n",
        "        if self.use_pydrive:\n",
        "            downloaded = self.drive.CreateFile({'id':file_id})\n",
        "            downloaded.FetchMetadata(fetch_all=True)\n",
        "            downloaded.GetContentFile(file_dst)\n",
        "        else:\n",
        "            !gdown --id $file_id -O $file_dst\n",
        "\n",
        "downloader = Downloader(download_with_pydrive)\n",
        "\n",
        "experiment_type = 'ffhq_encode' #@param ['ffhq_encode', 'cars_encode', 'horse_encode', 'church_encode']\n",
        "\n",
        "#@title Download\n",
        "MODEL_PATHS = {\n",
        "    \"ffhq_encode\": {\"id\": \"1cUv_reLE6k3604or78EranS7XzuVMWeO\", \"name\": \"e4e_ffhq_encode.pt\"},\n",
        "    \"cars_encode\": {\"id\": \"17faPqBce2m1AQeLCLHUVXaDfxMRU2QcV\", \"name\": \"e4e_cars_encode.pt\"},\n",
        "    \"horse_encode\": {\"id\": \"1TkLLnuX86B_BMo2ocYD0kX9kWh53rUVX\", \"name\": \"e4e_horse_encode.pt\"},\n",
        "    \"church_encode\": {\"id\": \"1-L0ZdnQLwtdy6-A_Ccgq5uNJGTqE7qBa\", \"name\": \"e4e_church_encode.pt\"}\n",
        "}\n",
        "\n",
        "path = MODEL_PATHS[experiment_type]\n",
        "downloader.download_file(file_id=path[\"id\"], file_name=path[\"name\"])\n",
        "\n",
        "EXPERIMENT_DATA_ARGS = {\n",
        "    \"ffhq_encode\": {\n",
        "        \"model_path\": \"pretrained_models/e4e_ffhq_encode.pt\",\n",
        "        \"image_path\": \"notebooks/images/input_img.jpg\"\n",
        "    },\n",
        "    \"cars_encode\": {\n",
        "        \"model_path\": \"pretrained_models/e4e_cars_encode.pt\",\n",
        "        \"image_path\": \"notebooks/images/car_img.jpg\"\n",
        "    },\n",
        "    \"horse_encode\": {\n",
        "        \"model_path\": \"pretrained_models/e4e_horse_encode.pt\",\n",
        "        \"image_path\": \"notebooks/images/horse_img.jpg\"\n",
        "    },\n",
        "    \"church_encode\": {\n",
        "        \"model_path\": \"pretrained_models/e4e_church_encode.pt\",\n",
        "        \"image_path\": \"notebooks/images/church_img.jpg\"\n",
        "    }\n",
        "\n",
        "}\n",
        "# Setup required image transformations\n",
        "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]\n",
        "if experiment_type == 'cars_encode':\n",
        "    EXPERIMENT_ARGS['transform'] = transforms.Compose([\n",
        "            transforms.Resize((192, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    resize_dims = (256, 192)\n",
        "else:\n",
        "    EXPERIMENT_ARGS['transform'] = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    resize_dims = (256, 256)\n",
        "\n",
        "if experiment_type == \"ffhq_encode\" and 'shape_predictor_68_face_landmarks.dat' not in os.listdir():\n",
        "    !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "    !bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "def run_alignment(image_path):\n",
        "  import dlib\n",
        "  predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "  aligned_image = align_face(filepath=image_path, predictor=predictor)\n",
        "#   print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
        "  return aligned_image\n",
        "\n",
        "def run_on_batch(inputs, net):\n",
        "    images, latents = net(inputs.to(\"cuda\").float(), randomize_noise=False, return_latents=True)\n",
        "    if experiment_type == 'cars_encode':\n",
        "        images = images[:, :, 32:224, :]\n",
        "    return images, latents\n",
        "\n",
        "def preprocess(image_path):\n",
        "    if experiment_type == \"ffhq_encode\":\n",
        "        input_image = run_alignment(image_path)\n",
        "    else:\n",
        "        original_image = Image.open(image_path)\n",
        "        original_image = original_image.convert(\"RGB\")\n",
        "        input_image = original_image\n",
        "\n",
        "    return input_image\n",
        "\n",
        "def inference(preprocessed_image):\n",
        "    img_transforms = EXPERIMENT_ARGS['transform']\n",
        "    transformed_image = img_transforms(preprocessed_image)\n",
        "    with torch.no_grad():\n",
        "        images, latents = run_on_batch(transformed_image.unsqueeze(0), net)\n",
        "        result_image, latent = images[0], latents[0]\n",
        "        # flatten the latent\n",
        "        latent = latent.flatten()\n",
        "\n",
        "    return result_image, latent\n",
        "\n",
        "def process(image_path):\n",
        "    preprocessed_image = preprocess(image_path)\n",
        "    result_image, latent = inference(preprocessed_image)\n",
        "    return result_image, latent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Pretrained Encoder"
      ],
      "metadata": {
        "id": "xbdm2gVykpJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "ckpt = torch.load(model_path, map_location='cpu')\n",
        "opts = ckpt['opts']\n",
        "# pprint.pprint(opts)  # Display full options used\n",
        "# update the training options\n",
        "opts['checkpoint_path'] = model_path\n",
        "opts= Namespace(**opts)\n",
        "net = pSp(opts)\n",
        "net.eval()\n",
        "net.cuda()\n",
        "print('Model successfully loaded!')"
      ],
      "metadata": {
        "id": "mdRJpLgsj7oZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fd7b15e-8f63-42f6-af6f-6e7413ccbdfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading e4e over the pSp framework from checkpoint: pretrained_models/e4e_ffhq_encode.pt\n",
            "Model successfully loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "pprint.pprint(opts)  # Display full options used"
      ],
      "metadata": {
        "id": "kcBNRUPOywQU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5596882-930e-42f7-94b6-8e278c6b2c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(exp_dir=None, dataset_type='ffhq_encode', encoder_type='Encoder4Editing', batch_size=8, test_batch_size=4, workers=8, test_workers=4, learning_rate=0.0001, optim_name='ranger', train_decoder=False, start_from_latent_avg=True, lpips_lambda=0.8, id_lambda=0.1, l2_lambda=1.0, stylegan_weights='', stylegan_size=1024, checkpoint_path='pretrained_models/e4e_ffhq_encode.pt', max_steps=300000, image_interval=100, board_interval=50, val_interval=10000, save_interval=200000, w_discriminator_lambda=0.1, w_discriminator_lr=2e-05, r1=10, d_reg_every=16, use_w_pool=True, w_pool_size=50, sub_exp_dir=None, delta_norm=2, delta_norm_lambda=0.0002, keep_optimizer=False, resume_training_from_ckpt=None, update_param_list=None, device='cuda:0', lpips_type='alex', progressive_steps=[0, 20000, 22000, 24000, 26000, 28000, 30000, 32000, 34000, 36000, 38000, 40000, 42000, 44000, 46000, 48000, 50000, 52000], progressive_start=20000, progressive_step_every=2000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oA2XYUtmFBx",
        "outputId": "c7f467d4-d429-441b-bdd5-95e9bb20368c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.random_projection import GaussianRandomProjection\n",
        "\n",
        "# Create a fixed projection matrix using Johnson–Lindenstrauss lemma\n",
        "def get_random_projection(original_dim=18 * 512, target_dim=512, seed=42):\n",
        "    rp = GaussianRandomProjection(n_components=target_dim, random_state=seed)\n",
        "    rp.fit(np.eye(original_dim))  # Fit on identity to get components\n",
        "    return rp.components_.T  # shape (original_dim, target_dim)\n",
        "\n",
        "# Apply the projection matrix\n",
        "def reduce_latent_dim(latent, W):\n",
        "    return latent @ W  # shape (512,)\n",
        "\n",
        "# Main dataset loading function\n",
        "def load_image_latent_dataset(base_path, transform_fn, projection_matrix, file_exts={'.jpg', '.png', '.jpeg', '.bmp', '.webp'}):\n",
        "    X = []\n",
        "    y = []\n",
        "    label_names = []\n",
        "\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        label = os.path.basename(root)\n",
        "        for file in files:\n",
        "            ext = os.path.splitext(file)[-1].lower()\n",
        "            if ext in file_exts:\n",
        "                full_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    _, latent = transform_fn(full_path)  # latent: (18 x 512,)\n",
        "                    latent_np = latent.to('cpu').numpy()\n",
        "                    reduced = reduce_latent_dim(latent_np, projection_matrix)\n",
        "                    X.append(reduced)\n",
        "                    label_names.append(label)\n",
        "                except Exception as e:\n",
        "                    print(f\"Skipping {full_path}: {e}\")\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(label_names)\n",
        "\n",
        "    return np.array(X), np.array(y_encoded), le\n",
        "\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Dataset\"\n",
        "projection_matrix = get_random_projection()\n",
        "X, y, label_encoder = load_image_latent_dataset(DATASET_PATH, transform_fn=process, projection_matrix=projection_matrix)\n"
      ],
      "metadata": {
        "id": "oUmwLm8FkHpH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c331f1a7-f363-4ecb-8ae5-4c8cd3195ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3320125138.py:112: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape,y.shape"
      ],
      "metadata": {
        "id": "PdI9LdBjS0-E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e42a0fd-a779-4765-af06-f0fd77aec93e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((499, 512), (499,))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder.classes_"
      ],
      "metadata": {
        "id": "8PHBV3dZTDdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a3ac7c-1cf5-4c15-9fb8-8b28fba3de0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['African', 'Asian', 'Caucasian', 'Hispanic', 'Middle Eastern'],\n",
              "      dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "NOeZVOHjarQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    # Load latents and labels\n",
        "    X, y, label_encoder = load_image_latent_dataset(DATASET_PATH, transform_fn=process, projection_matrix=projection_matrix)\n",
        "    print(f\"X and Y shape: {X.shape, y.shape}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# Call the function and get data\n",
        "X, y= load_data()\n",
        "\n",
        "print(X[:2])  # first two feature vectors\n",
        "print(y[:2])  # first two labels"
      ],
      "metadata": {
        "id": "4sqdhvvGcj9G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "572d408f-8d75-431c-e1e6-1cddbe253c2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3320125138.py:112: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  img = PIL.Image.fromarray(np.uint8(np.clip(np.rint(img), 0, 255)), 'RGB')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X and Y shape: ((499, 512), (499,))\n",
            "[[-3.22221695  2.43548935 -2.35343109 ... -2.02379906 -0.98668709\n",
            "   0.53092986]\n",
            " [-0.41097158  0.43074532  2.73369143 ...  0.49759565  0.28030269\n",
            "   0.18811426]]\n",
            "[2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Split data\n",
        "def split_data(X, y, val_size=0.2):\n",
        "    \"\"\"\n",
        "    Split data into train, val, and test sets.\n",
        "    \"\"\"\n",
        "    X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "    return X_train, X_val, y_train, y_val\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = split_data(X, y, val_size=0.2)\n",
        "\n",
        "print(f\"Train shape: {X_train.shape}, Val shape: {X_val.shape}\")"
      ],
      "metadata": {
        "id": "QSw8wQiGu1x4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b10727-7950-4be8-9c30-0e200421fbf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (399, 512), Val shape: (100, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "\n",
        "def fit_knn_without_pca(X_train, y_train, X_val, y_val, n_neighbors=20):\n",
        "    \"\"\"\n",
        "    Fit KNN classifier and return the fitted model.\n",
        "    Also plots the error rate vs K.\n",
        "    \"\"\"\n",
        "    error_rate = []\n",
        "\n",
        "    for i in range(1, n_neighbors+1):\n",
        "        knn = KNeighborsClassifier(n_neighbors=i)\n",
        "        knn.fit(X_train, y_train)\n",
        "        pred_i = knn.predict(X_val)\n",
        "        error_rate.append(np.mean(pred_i != y_val))\n",
        "\n",
        "    # Return model with best K\n",
        "    best_k = error_rate.index(min(error_rate)) + 1\n",
        "    print(f\"Best K: {best_k} with error rate {min(error_rate):.4f}\")\n",
        "\n",
        "    best_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
        "    best_knn.fit(X_train, y_train)\n",
        "    return best_knn\n",
        "knn_model_without_pca=fit_knn_without_pca(X_train, y_train, X_val, y_val, n_neighbors=20)"
      ],
      "metadata": {
        "id": "9pU-PnjyvTrh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "878839f1-9e0b-431b-fcec-da977776672d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best K: 2 with error rate 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3a: Fit PCA on training data\n",
        "def fit_pca(X_train, variance_threshold=0.99):\n",
        "\n",
        "    \"\"\"\n",
        "    Fit PCA with enough components to retain 99% variance.\n",
        "    Return the fitted PCA object and transformed data.\n",
        "    \"\"\"\n",
        "    pca = PCA(n_components=variance_threshold, svd_solver='full')\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    print(f\"Original shape: {X_train.shape}\")\n",
        "    print(f\"Reduced shape: {X_train_pca.shape}\")\n",
        "    print(f\"Explained variance ratio sum: {pca.explained_variance_ratio_.sum():.4f}\")\n",
        "    return pca, X_train_pca\n",
        "fit_pca=fit_pca(X_train)"
      ],
      "metadata": {
        "id": "beb-u4XF1QRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f5c9bdf-3ad1-4c77-e0fd-fa30986eab8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (399, 512)\n",
            "Reduced shape: (399, 254)\n",
            "Explained variance ratio sum: 0.9902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3b: Fit KNN classifier\n",
        "def fit_knn_with_pca(X_train_pca, y_train, X_val, y_val, n_neighbors=20):\n",
        "    \"\"\"\n",
        "    Fit KNN classifier on PCA-reduced data.\n",
        "    Return the fitted model.\n",
        "    \"\"\"\n",
        "    error_rate=[]\n",
        "    for i in range(1, n_neighbors+1):\n",
        "        knn = KNeighborsClassifier(n_neighbors=i)\n",
        "        knn.fit(X_train_pca, y_train)\n",
        "        pred_i = knn.predict(X_val)\n",
        "        error_rate.append(np.mean(pred_i != y_val))\n",
        "\n",
        "\n",
        "    # Return model with best K\n",
        "    best_k = error_rate.index(min(error_rate)) + 1\n",
        "    print(f\"Best K: {best_k} with error rate {min(error_rate):.4f}\")\n",
        "\n",
        "    best_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
        "    best_knn.fit(X_train_pca, y_train)\n",
        "    return best_knn\n",
        "knn_model_without_pca=fit_knn_without_pca(X_train, y_train, X_val, y_val, n_neighbors=20)"
      ],
      "metadata": {
        "id": "F37lOpBY2SMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014a6db5-d3d0-4a1a-bac7-63aa92a62b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best K: 2 with error rate 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Evaluate model on validation set\n",
        "def evaluate(models, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Predict on val and return accuracy.\n",
        "    \"\"\"\n",
        "    val_accuracies=[]\n",
        "    for model in models:\n",
        "        val_accuracy = model.score(X_val, y_val)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "    return val_accuracies\n",
        "\n"
      ],
      "metadata": {
        "id": "FLy-qZ5C25I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Compare both approaches and pick the better one\n",
        "def compare_models(X_train, y_train, X_val, y_val):\n",
        "    # Fit KNN without PCA\n",
        "    knn_without_pca = fit_knn_without_pca(X_train, y_train, X_val, y_val, n_neighbors=20)\n",
        "\n",
        "    # Fit PCA and transform validation data\n",
        "    pca, X_train_pca = fit_pca(X_train)\n",
        "    X_val_pca = pca.transform(X_val)\n",
        "\n",
        "    # Fit KNN with PCA\n",
        "    knn_with_pca = fit_knn_with_pca(X_train_pca, y_train, X_val_pca, y_val, n_neighbors=20)\n",
        "\n",
        "    models = [knn_without_pca, knn_with_pca]\n",
        "    val_data = [X_val, X_val_pca]\n",
        "    val_accuracies = [evaluate([models[i]], [val_data[i]], y_val)[0] for i in range(len(models))]\n",
        "\n",
        "    print(f\"Validation accuracy without PCA: {val_accuracies[0]:.4f}\")\n",
        "    print(f\"Validation accuracy with PCA: {val_accuracies[1]:.4f}\")\n",
        "\n",
        "    best_model_idx = np.argmax(val_accuracies)\n",
        "    best_model = models[best_model_idx]\n",
        "    return best_model, pca if best_model_idx == 1 else None\n",
        "\n",
        "best_model, best_pca = compare_models(X_train, y_train, X_val, y_val)"
      ],
      "metadata": {
        "id": "-8Qbru2w3ygx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d3b418f-9ab4-482f-94b8-908b399ef2f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best K: 2 with error rate 0.5000\n",
            "Original shape: (399, 512)\n",
            "Reduced shape: (399, 254)\n",
            "Explained variance ratio sum: 0.9902\n",
            "Best K: 2 with error rate 0.5000\n",
            "Validation accuracy without PCA: 0.5000\n",
            "Validation accuracy with PCA: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(image_path, transform_fn, projection_matrix, model, label_encoder, pca=None):\n",
        "    \"\"\"\n",
        "    Given a new image path, return the predicted label and class.\n",
        "\n",
        "    Steps:\n",
        "    - Load and encode image using transform_fn\n",
        "    - Project to 512D using the given matrix\n",
        "    - If PCA was used, apply the same fitted PCA\n",
        "    - Predict using the given model\n",
        "    - Decode label using label_encoder\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Load and encode image\n",
        "    _, latent = transform_fn(image_path)  # latent is the 18 x 512 vector\n",
        "\n",
        "    # Step 2: Project latent to 512D using the given matrix\n",
        "    latent_np = latent.cpu().numpy()  # Ensure the latent is on CPU and convert to numpy\n",
        "    reduced_latent = reduce_latent_dim(latent_np, projection_matrix)\n",
        "\n",
        "    # Step 3: Apply PCA if it was used during training\n",
        "    if pca is not None:\n",
        "        reduced_latent_for_prediction = pca.transform([reduced_latent])  # Apply PCA transformation\n",
        "    else:\n",
        "        reduced_latent_for_prediction = reduced_latent.reshape(1, -1) # Reshape for the model if PCA was not applied\n",
        "\n",
        "\n",
        "    # Step 4: Make a prediction using the model (KNN)\n",
        "    predicted_class = model.predict(reduced_latent_for_prediction)\n",
        "\n",
        "    # Step 5: Decode the predicted label using the label_encoder\n",
        "    predicted_label = label_encoder.inverse_transform(predicted_class)\n",
        "\n",
        "    return predicted_label[0], predicted_class[0] # Return the predicted label and class"
      ],
      "metadata": {
        "id": "PVBPcjInthn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/content/african.jpg\"\n",
        "\n",
        "# Use the best_model and best_pca returned by compare_models\n",
        "predicted_label_with_best_model, predicted_class_with_best_model = infer(image_path, transform_fn=process, projection_matrix=projection_matrix, model=best_model, label_encoder=label_encoder, pca=best_pca)\n",
        "\n",
        "print(f\"Predicted label with the best model: {predicted_label_with_best_model}\")\n",
        "print(f\"Predicted class (encoded) with the best model: {predicted_class_with_best_model}\")"
      ],
      "metadata": {
        "id": "sDS3msrNwklh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ac11828-1e17-4f8d-d3ab-fd6e5c93ffaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label with the best model: African\n",
            "Predicted class (encoded) with the best model: 0\n"
          ]
        }
      ]
    }
  ]
}